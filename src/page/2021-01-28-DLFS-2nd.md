---
layout: post
title: "밑바닥부터 시작하는 딥러닝 2"
date: 2021-01-28 22:53:55 +0900
author: JunYoung
categories: CS ML
tags: Machine_Learning Deep_Learning Python
math: true
---

## Ch4

기본적인 내용은 Gradient Descent(Stochastic Gradient Descent)를 이용하여
손실 함수를 최소화하여 신경망을 학습시키는 것이다.

$$
x_{n + 1} = x_{n} - \eta \left.\frac{ df}{dx} \right|_{x = x_n}
$$

을 반복하여 주어진 함수 $$ f $$를 최소화하는 $$ x $$를 찾게 된다.

머신 러닝의 경우, 모델의 parameter를 변경시키며 손실 함수를 최소화하는 것이므로
기울기는 gradient 벡터의 형태가 된다.

gradient는 수치 해석을 통해 다음과 같이 계산할 수 있다.

```python
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)

    for i in range(x.size):
        tmp_val = x[i]
        x[i] = float(tmp_val) + h
        fxh1 = f(x)
        x[i] = float(tmp_val) - h
        fxh2 = f(x)

        grad[i] = (fxh1 - fxh2) / (2 * h)
        x[i] = tmp_val

    return grad
```

각 편도함수를 대칭미분계수를 사용하여 근사한다.
(공학수학 시간에 배웠던 걸 생각하면 RK4 와 같은 방법이면 더 정확하게 근사할 수 있을 것 같다.)
step size나 learning rate 잡는 건 함수에 따라 달라질 수 있음을 유의하자.

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']

        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)

        return y

    def loss(self, x, t):
        y = self.predict(x)

        return entropy(y, t)


    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)


        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads
```

2단계의 neural net을 다음과 같이 만들었다.

처음 볼때 저 gradient 함수가 좀 헷갈렸는데(loss_W가 상수함수인데 gradient가 0이 되지 않는 건가? 싶었음), self.params를 gradient에 인자로 넘겨주기 때문에, numerical_gradient 함수에서 self.params를 수정하면 객체의
parameter가 수정되어 loss_W가 다른 값을 리턴하게 된다.

그 뒤론 딱히 어려운 내용은 없다.
다만 numerical_gradient를 그대로 쓰기엔 데이터의 크기가 커지면 너무 느리다.
다음 장에서 다른 방법을 배운다고 한다.

활성화 함수의 선택이나 learning rate의 선택, 신경망 층의 깊이와 parameter
개수를 선택하는 과정 등이 궁금한데, 이 책에 나오려나? 모르겠다.

이 책의 이의는 이거 읽는다고 전문성을 가지기보다는, 좀 더 전문적인 지식을 공부하기
위한 base를 빠르게 쌓는 데에 있는 것 같다. 설명이 친절한 점은 좋다.

여담. parameter는 2차원 행렬로 되어 있어 numerical_gradient함수를 좀 수정해야 한다.
근데 책에는 전혀 언급이 없어서 에러메세지 보고 알았음...
