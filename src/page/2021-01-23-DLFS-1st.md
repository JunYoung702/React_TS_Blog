---
layout: post
title: "밑바닥부터 시작하는 딥러닝 1"
date: 2021-01-23 01:40:55 +0900
author: JunYoung
categories: CS ML
tags: Machine_Learning Deep_Learning Python
math: true
---

<a href ="https://github.com/JunYoung702/DeepLearningFromScratch">Github Repo</a>

공부한걸 기록해놓기위한 포스트.

공부할 때 든 생각을 안 적어놓으면 까먹는 경우가 많아서 매일 기록해보려고 시작했다.
근데 당분간은 밀리는거 몰아서 올릴수도있음

## Ch1

그냥 파이썬 문법 + numpy + matplotlib 설명

딱히 특별한건 없고 이건 기억해두면 좋겠다 싶은 간단 문법이다.

```python
import numpy as np
x = np.array([51, 55, 14, 19, 0, 4])
>>> x > 15
array([True, True, False, True, False, False])
>>> x[x > 15]
array([51, 55, 19])
```

## Ch2

### 퍼셉트론:

$$
y = \left\{ \begin{array}{rl}
0 & (w_1 x_1 + w_2 x_2 + b \leq 0) \\
1 & (w_1 x_1 + w_2 x_2 + b > 0)  \end{array} \right.
$$

입력 $$x_1,\; x_2$$ 에 가중치를 곱해 더한 후 편차를 더해서 그 값이
$$0$$ 이상이면 $$1$$을, 아니면 $$ 0 $$을 리턴한다. (일종의 Threshold 개념)

기하학적으로는 $$\mathbb{R}^2$$평면에서 어떤 직선에 의해 나누어지는 경계를 생각할 수 있다.

XOR 게이트는 퍼셉트론 하나만으로는 표현이 불가능하다.

(직선 하나로 $$(0, 0) , (1, 1)$$ 과 $$(1, 0), (0, 1)$$을 나눌 수 없다.)

두 개 이상의 퍼셉트론으로 표현할 수 있다.

```
(AND(NAND(x1, x2), OR(x1, x2)))
```

로 표현이 가능하다.

퍼셉트론으로 논리회로의 기본 게이트를 만들 수 있으므로 이론상 컴퓨터를 표현할 수 있다.

## Ch3

### 활성화 함수

Ch2에서 나온 퍼셉트론은

$$
y  = h(b + w_1 x_1 + w_2 x_2), \; h(x) = \left\{ \begin{array}{rl}
    0 & (x \leq 0)\\
    1 & (x > 0)
\end{array} \right.
$$

으로 이해할 수 있다. 이때 $$ h(x) $$ 를 활성화 함수라고 부른다.

비선형 활성화 함수가 없으면 퍼셉트론을 아무리 많이 쌓아봤자 일차함수의 합성은 일차함수이므로
선형 함수밖에 표현할 수 없다.

### 활성화 함수에서의 오버플로우 주의

softmax 함수 $$ y_k = \frac{\displaystyle \exp(a_k)}{\displaystyle \sum \exp(a_i)} $$ 를 예시로 들면, $$exp(a_k)$$에 너무 큰 값이 들어가면 오버플러우가 발생한다.

이를 다음과 같이 조정하여 해결할 수 있다.

$$
\displaystyle  y_k = \frac{\exp(a_k)}{\sum \exp(a_i)} = \frac{\exp(a_k - c)}{\sum \exp(a_i - c)} \;(c = \max_i (a_i))
$$

(이 주제 관련해서 더 자세한 내용은 <a href ="https://www.deeplearningbook.org/">\<심층 학습\>(이안 굿펠로, 요수아 벤지오, 에런 쿠빌)</a>에 있던
것으로 기억한다.)

### softmax 함수

softmax함수의 합은 $$1$$이므로 결괏값을 확률로 해석할 수 있다. 분류 문제 등에서 그렇게 해석하는 예시가 있다.

### 행렬곱 연산으로 처리

가중치 계산이 결국 내적이기 때문에 numpy의 행렬곱 연산을 이용하여 벡터로 주어지는 데이터의 연산을
쉽게 처리할 수 있다.

$$n$$ 개의 숫자로 이루어진 input이 다음과 같이 주어진다고 생각하자.

```python
X = np.array([x1, x2, ... , x_n])
```

다음 은닉층(혹은 출력층)이 $$m$$개의 노드로 이루어져 있다고 생각하면
가중치 행렬 W는 다음과 같이 쓸 수 있다.

```python
W = np.array([ [w11, w12, ... , w1m], [w21, ... w2m], ..., [wn1, ... , wnm]  ])
```

이는

$$
W =  \left( \begin{array}{cccc}
    w_{11} & w_{12} & \cdots & w_{1m} \\
    w_{21} & w_{22} & \cdots & w_{2m} \\
    \vdots & \vdots   & \ddots     & \vdots \\
    w_{n1} & w_{n2} & \cdots & v_{nm}
  \end{array}\right)
$$

와 같다.

$$
X = (x_1, x_2, \cdots , x_n)
$$

$$
b = (b_1, b_2, \cdots , b_m)
$$

이라 하면

$$
Y = X \cdot W + b
$$

라 할 수 있고, 여기에 활성화 함수를 적용시키면 각 층에서의 신경망 계산을 할 수 있다.

(사실 식 모양도 그렇고 선형대수를 생각하면 전체 식을 전치시켜서 $$ m \times n $$ 행렬 $$W$$와
열벡터 $$X$$ 를 곱하고 열벡터 $$b$$를 더하는 게 자연스럽지 않나 싶은데, 아마 numpy의
벡터와 행렬의 형태 때문에 저런 형태로 쓰는 것 같다.)

(이 관점에서 볼 때는 W = [v1, v2, ... , vn] 에 대하여 각각의 vi 를 x[i]가 다음 층으로 향할 때
각각의 뉴런에 대한 x[i]의 가중치를 모은 벡터라고 기억하면 좀 더 기억하기 쉬운 것 같다.)

### 데이터 전처리

데이터 전처리에 대한 얘기도 잠깐 나오는데(mnist 데이터의 정규화), 그쪽 관련된 내용은
지금 보고 있는 다른 책인 <a href = "https://dataninja.me/">\<따라 하며 배우는 데이터 과학\></a>
에서 좀 더 자세히 배울 수 있을 것 같다.
